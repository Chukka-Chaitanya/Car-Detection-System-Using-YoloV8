# üöó YOLOv8 Car Detection and Deployment to Raspberry Pi

A real-time car detection model trained with YOLOv8 and optimized for edge deployment. This project demonstrates the end-to-end workflow: from data preparation and training in PyTorch to quantization with TensorFlow Lite for efficient inference on a Raspberry Pi.

---

## üìä Results

Here is an example of the trained model detecting a car. The final quantized model achieves high-speed inference on edge devices with minimal accuracy loss.

![Detection Result](results/detection_example.png)

---

## üõ†Ô∏è Technology Stack

* **Development:**
    * Python 3.10+
    * PyTorch & Ultralytics YOLOv8 (for training)
    * ONNX (for model exporting)
    * TensorFlow & TensorFlow Lite (for quantization)
    * OpenCV & Kaggle API
* **Deployment:**
    * Raspberry Pi 4 (or newer)
    * `tflite-runtime`

---

## üì¶ Getting Started

Follow these instructions to set up the project on your local machine for training and quantization.

### Prerequisites

* Python 3.10 or newer installed.

### Installation & Setup

1.  **Create a `requirements.txt` file**
    Create a file named `requirements.txt` and add the following libraries. Note the addition of `tensorflow` and `onnx`.
    ```txt
    ultralytics
    kaggle
    opencv-python
    tqdm
    tensorflow
    onnx
    ```

2.  **Create and Activate a Virtual Environment**
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set Up Kaggle API**
    Place your `kaggle.json` API key in `C:\Users\<Your-Username>\.kaggle\` to allow dataset downloads.

---

## üöÄ Project Workflow

The project is divided into several steps, from data preparation to deployment.

1.  **Prepare the Dataset**
    This script downloads the dataset and converts the labels from `.xml` to YOLO's `.txt` format.
    ```bash
    python prepare_dataset.py 
    ```

2.  **Train the YOLOv8 Model**
    This script starts the YOLOv8 training process. The best-trained model will be saved as a `.pt` file (e.g., in `runs/detect/train/weights/best.pt`).
    ```bash
    python train.py
    ```

3.  **Export the Model to ONNX**
    TensorFlow Lite cannot directly read PyTorch `.pt` files. We must first export our trained model to the standard ONNX format.
    ```bash
    python export.py --weights path/to/your/best.pt
    ```
    4.  **Quantize the Model to TensorFlow Lite**
    This script converts the `.onnx` model to a quantized `int8.tflite` model, ready for the Raspberry Pi.
    ```bash
    python quantize.py --onnx_model path/to/your/best.onnx
    ```
    ---

## ü•ß Deployment on Raspberry Pi

The final `int8.tflite` model is optimized to run efficiently on your Raspberry Pi.

1.  **Transfer the Model**
    Copy your `model_quantized_int8.tflite` file and any necessary class label files to your Raspberry Pi.

2.  **Install `tflite-runtime` on the Pi**
    On your Raspberry Pi's terminal, install the lightweight TFLite interpreter.
    ```bash
    pip install tflite-runtime
    ```

3.  **Run Inference on the Pi**
    Use a simple Python script on your Pi to load the quantized model and run detection on a camera feed or image.
    ```python
    # Example script: detect_pi.py
    import tflite_runtime.interpreter as tflite
    import numpy as np
    import cv2

    # Load the TFLite model
    interpreter = tflite.Interpreter(model_path="model_quantized_int8.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensor details
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Load and preprocess image from camera or file
    # ... your image loading and preprocessing code ...

    # Set the input tensor
    interpreter.set_tensor(input_details[0]['index'], your_preprocessed_image)

    # Run inference
    interpreter.invoke()

    # Get the results
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print("Inference successful!")
    ```
